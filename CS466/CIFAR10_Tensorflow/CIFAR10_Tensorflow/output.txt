########################################################################################
########################################################################################
########################################################################################
2017-04-30 23:34:51.979702: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-04-30 23:35:11.082622: step 0, loss = 4.67 (7591.2 examples/sec; 0.017 sec/batch)
2017-04-30 23:35:14.880885: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-01 14:03:23.369124: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-01 14:03:43.588935: step 0, loss = 4.67 (7318.6 examples/sec; 0.017 sec/batch)
2017-05-01 14:03:47.274226: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-01 14:07:30.386224: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-01 14:07:49.082088: step 0, loss = 4.68 (7771.3 examples/sec; 0.016 sec/batch)
2017-05-01 14:08:28.672942: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-01 15:41:47.989052: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-01 15:42:08.087093: step 0, loss = 4.68 (7200.1 examples/sec; 0.018 sec/batch)
2017-05-01 15:42:47.957178: DONE
########################################################################################
########################################################################################
########################################################################################
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 03:10:50.709811: step 0, loss = 4.68 (8012.4 examples/sec; 0.016 sec/batch)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 05:11:27.176241: step 0, loss = 6.38 (1982.8 examples/sec; 0.065 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-02 19:42:16.698996: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 19:42:26.526405: step 0, loss = 6.38 (1818.8 examples/sec; 0.070 sec/batch)
2017-05-02 19:44:14.946251: step 100, loss = 5.90 (118.1 examples/sec; 1.084 sec/batch)
2017-05-02 19:46:18.165279: step 200, loss = 5.10 (103.9 examples/sec; 1.232 sec/batch)
2017-05-02 19:48:03.199919: precision @ 1 = 0.545
2017-05-02 19:48:03.914295: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 19:49:23.382655: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 19:51:44.222357: precision @ 1 = 0.566
2017-05-02 19:51:44.827254: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:10:34.805595: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:14:53.416338: Running on MSI...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:16:05.473184: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 23:16:14.502855: step 0, loss = 6.38 (1902.7 examples/sec; 0.067 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-02 23:18:10.791801: Running on MSI...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 23:19:07.150263: precision @ 1 = 0.495
2017-05-02 23:19:07.848757: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:22:25.640494: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:23:34.025582: precision @ 1 = 0.000
2017-05-02 13:23:34.447543: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:36:30.237930: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:37:32.690747: precision @ 1 = 0.498
2017-05-02 13:37:33.090898: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:38:09.926342: Running on server...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:39:30.727659: Running on server...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:39:59.824929: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:41:56.159177: precision @ 1 = 0.512
2017-05-02 13:41:56.551452: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:43:31.491927: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:45:20.958808: precision @ 1 = 0.000
2017-05-02 13:45:21.373092: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:48:43.781418: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 13:50:32.484337: precision @ 1 = 0.487
2017-05-02 13:50:32.883299: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:53:59.767160: Running on server...
########################################################################################
########################################################################################
########################################################################################
2017-05-02 13:54:29.556481: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 14:50:37.834941: precision @ 1 = 0.834
2017-05-02 14:50:38.205392: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 16:33:44.146097: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 16:34:14.374875: step 0, loss = 6.38 (12.4 examples/sec; 10.295 sec/batch)
2017-05-02 16:34:49.113811: step 100, loss = 5.64 (1429.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:35:14.933199: step 200, loss = 4.78 (1252.8 examples/sec; 0.102 sec/batch)
2017-05-02 16:35:38.773353: step 300, loss = 4.65 (1268.5 examples/sec; 0.101 sec/batch)
2017-05-02 16:35:59.403723: step 400, loss = 4.35 (1201.6 examples/sec; 0.107 sec/batch)
2017-05-02 16:36:20.196845: step 500, loss = 3.86 (1295.0 examples/sec; 0.099 sec/batch)
2017-05-02 16:36:39.009702: step 600, loss = 3.71 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:36:59.917474: step 700, loss = 3.27 (1318.8 examples/sec; 0.097 sec/batch)
2017-05-02 16:37:22.136216: step 800, loss = 3.04 (1358.8 examples/sec; 0.094 sec/batch)
2017-05-02 16:37:46.449561: step 900, loss = 2.99 (1377.9 examples/sec; 0.093 sec/batch)
2017-05-02 16:38:15.421375: precision @ 1 = 0.713
2017-05-02 16:38:15.933319: DONE
########################################################################################
########################################################################################
########################################################################################
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:31:56.498448: Running on server...
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:32:30.506995: step 0, loss = 6.37 (10.6 examples/sec; 12.072 sec/batch)
2017-05-02 17:33:10.174777: step 100, loss = 5.51 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-02 17:33:34.751237: step 200, loss = 5.03 (1362.7 examples/sec; 0.094 sec/batch)
2017-05-02 17:33:54.359336: step 300, loss = 4.62 (1323.7 examples/sec; 0.097 sec/batch)
2017-05-02 17:34:13.587034: step 400, loss = 4.30 (1388.9 examples/sec; 0.092 sec/batch)
2017-05-02 17:34:33.441458: step 500, loss = 3.97 (1386.1 examples/sec; 0.092 sec/batch)
2017-05-02 17:34:52.375089: step 600, loss = 3.54 (1453.1 examples/sec; 0.088 sec/batch)
2017-05-02 17:35:11.520342: step 700, loss = 3.48 (1278.9 examples/sec; 0.100 sec/batch)
2017-05-02 17:35:33.253571: step 800, loss = 3.34 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 17:35:51.299611: step 900, loss = 2.84 (1471.6 examples/sec; 0.087 sec/batch)
2017-05-02 17:36:17.918064: precision @ 1 = 0.711
2017-05-02 17:36:18.330605: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:45:31.199255: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:46:06.482917: step 0, loss = 6.38 (9.9 examples/sec; 12.885 sec/batch)
2017-05-02 17:46:27.050001: step 100, loss = 5.71 (1423.8 examples/sec; 0.090 sec/batch)
2017-05-02 17:46:47.448390: step 200, loss = 5.21 (1389.7 examples/sec; 0.092 sec/batch)
2017-05-02 17:47:05.692106: step 300, loss = 4.49 (1399.2 examples/sec; 0.091 sec/batch)
2017-05-02 17:47:23.638216: step 400, loss = 4.35 (1487.6 examples/sec; 0.086 sec/batch)
2017-05-02 17:47:42.062470: step 500, loss = 3.86 (1474.6 examples/sec; 0.087 sec/batch)
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:50:15.249141: Running on server...
The experiment details:
max_steps = 1000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:50:48.026697: step 0, loss = 6.38 (11.5 examples/sec; 11.083 sec/batch)
2017-05-02 17:51:11.178423: step 100, loss = 5.64 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 17:51:30.233787: step 200, loss = 5.18 (1453.2 examples/sec; 0.088 sec/batch)
2017-05-02 17:51:49.429536: step 300, loss = 4.51 (1457.5 examples/sec; 0.088 sec/batch)
2017-05-02 17:52:07.644800: step 400, loss = 4.27 (1426.9 examples/sec; 0.090 sec/batch)
2017-05-02 17:52:27.556866: step 500, loss = 4.13 (1436.4 examples/sec; 0.089 sec/batch)
2017-05-02 17:52:53.431334: step 600, loss = 3.91 (1453.8 examples/sec; 0.088 sec/batch)
2017-05-02 17:53:12.830494: step 700, loss = 3.49 (1266.9 examples/sec; 0.101 sec/batch)
2017-05-02 17:53:31.149599: step 800, loss = 3.17 (1446.2 examples/sec; 0.089 sec/batch)
2017-05-02 17:53:49.069866: step 900, loss = 2.99 (1513.2 examples/sec; 0.085 sec/batch)
2017-05-02 17:54:13.430937: precision @ 1 = 0.710
2017-05-02 17:54:13.822117: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:58:43.667286: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 17:59:12.759130: step 0, loss = 6.38 (11.9 examples/sec; 10.756 sec/batch)
2017-05-02 17:59:41.542099: precision @ 1 = 0.446
2017-05-02 17:59:41.957548: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 17:59:42.022295: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
########################################################################################
########################################################################################
########################################################################################
2017-05-02 18:00:39.185265: Running on server...
The experiment details:
max_steps = 100 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 18:01:15.921001: step 0, loss = 6.38 (8.9 examples/sec; 14.317 sec/batch)
2017-05-02 18:01:56.185569: precision @ 1 = 0.446
2017-05-02 18:01:57.050571: DONE
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
########################################################################################
2017-05-02 18:17:44.470974: Running on server...
The experiment details:
max_steps = 30000 log_frequency = 100 num_gpus = 2
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-02 18:18:09.650383: step 0, loss = 6.37 (14.5 examples/sec; 8.802 sec/batch)
2017-05-02 18:18:29.276130: step 100, loss = 5.62 (1515.9 examples/sec; 0.084 sec/batch)
2017-05-02 18:18:46.677021: step 200, loss = 4.93 (1508.0 examples/sec; 0.085 sec/batch)
2017-05-02 18:19:04.087997: step 300, loss = 4.67 (1505.4 examples/sec; 0.085 sec/batch)
2017-05-02 18:19:21.195210: step 400, loss = 4.16 (1525.5 examples/sec; 0.084 sec/batch)
2017-05-02 18:19:38.585594: step 500, loss = 3.85 (1481.9 examples/sec; 0.086 sec/batch)
2017-05-02 18:19:55.776764: step 600, loss = 3.53 (1460.8 examples/sec; 0.088 sec/batch)
2017-05-02 18:20:14.028559: step 700, loss = 3.56 (1498.4 examples/sec; 0.085 sec/batch)
2017-05-02 18:20:32.181412: step 800, loss = 3.25 (1507.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:20:49.636057: step 900, loss = 2.88 (1436.0 examples/sec; 0.089 sec/batch)
2017-05-02 18:21:07.140694: step 1000, loss = 2.87 (1514.2 examples/sec; 0.085 sec/batch)
2017-05-02 18:21:26.605593: step 1100, loss = 2.51 (476.3 examples/sec; 0.269 sec/batch)
2017-05-02 18:21:43.848308: step 1200, loss = 2.41 (1505.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:22:01.567575: step 1300, loss = 2.13 (1464.1 examples/sec; 0.087 sec/batch)
2017-05-02 18:22:19.052845: step 1400, loss = 2.13 (1506.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:22:37.700160: step 1500, loss = 2.17 (1431.9 examples/sec; 0.089 sec/batch)
2017-05-02 18:22:55.126680: step 1600, loss = 1.96 (1505.5 examples/sec; 0.085 sec/batch)
2017-05-02 18:23:12.911478: step 1700, loss = 1.89 (1392.8 examples/sec; 0.092 sec/batch)
2017-05-02 18:23:30.338358: step 1800, loss = 1.69 (1503.2 examples/sec; 0.085 sec/batch)
2017-05-02 18:23:47.984820: step 1900, loss = 1.64 (1506.9 examples/sec; 0.085 sec/batch)
2017-05-02 18:24:05.374922: step 2000, loss = 1.43 (1506.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:24:24.677808: step 2100, loss = 1.56 (1491.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:24:42.103722: step 2200, loss = 1.39 (1519.1 examples/sec; 0.084 sec/batch)
2017-05-02 18:24:59.442160: step 2300, loss = 1.17 (1492.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:25:17.050851: step 2400, loss = 1.27 (1478.5 examples/sec; 0.087 sec/batch)
2017-05-02 18:25:34.731751: step 2500, loss = 1.17 (1531.8 examples/sec; 0.084 sec/batch)
2017-05-02 18:25:52.045027: step 2600, loss = 1.10 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 18:26:09.913263: step 2700, loss = 0.96 (1474.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:26:27.471754: step 2800, loss = 0.96 (1511.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:26:44.964049: step 2900, loss = 0.90 (1491.2 examples/sec; 0.086 sec/batch)
2017-05-02 18:27:02.746464: step 3000, loss = 0.92 (1490.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:27:22.314915: step 3100, loss = 0.91 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-02 18:27:39.791499: step 3200, loss = 0.86 (1514.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:27:57.821654: step 3300, loss = 0.94 (1477.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:28:15.275126: step 3400, loss = 0.86 (1507.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:28:32.697662: step 3500, loss = 0.77 (1479.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:28:50.135750: step 3600, loss = 0.94 (1529.0 examples/sec; 0.084 sec/batch)
2017-05-02 18:29:07.664664: step 3700, loss = 0.82 (1466.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:29:25.337218: step 3800, loss = 0.73 (1367.5 examples/sec; 0.094 sec/batch)
2017-05-02 18:29:42.958317: step 3900, loss = 0.68 (1465.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:30:00.772204: step 4000, loss = 0.71 (1507.0 examples/sec; 0.085 sec/batch)
2017-05-02 18:30:20.193414: step 4100, loss = 0.94 (1487.9 examples/sec; 0.086 sec/batch)
2017-05-02 18:30:37.958400: step 4200, loss = 0.70 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-02 18:30:55.555286: step 4300, loss = 0.58 (1518.7 examples/sec; 0.084 sec/batch)
2017-05-02 18:31:13.060451: step 4400, loss = 0.56 (1494.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:31:30.545714: step 4500, loss = 0.65 (1471.5 examples/sec; 0.087 sec/batch)
2017-05-02 18:31:48.904718: step 4600, loss = 0.53 (1499.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:32:06.706775: step 4700, loss = 0.46 (1522.6 examples/sec; 0.084 sec/batch)
2017-05-02 18:32:24.281454: step 4800, loss = 0.62 (1470.5 examples/sec; 0.087 sec/batch)
2017-05-02 18:32:42.161571: step 4900, loss = 0.51 (1480.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:32:59.939388: step 5000, loss = 0.61 (1472.7 examples/sec; 0.087 sec/batch)
2017-05-02 18:33:19.270810: step 5100, loss = 0.56 (1512.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:33:37.044476: step 5200, loss = 0.51 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:33:54.593386: step 5300, loss = 0.49 (1487.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:34:13.406068: step 5400, loss = 0.44 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-02 18:34:30.918694: step 5500, loss = 0.57 (1484.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:34:48.392447: step 5600, loss = 0.49 (1464.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:35:05.878322: step 5700, loss = 0.44 (1510.8 examples/sec; 0.085 sec/batch)
2017-05-02 18:35:23.342209: step 5800, loss = 0.52 (1533.5 examples/sec; 0.083 sec/batch)
2017-05-02 18:35:40.737003: step 5900, loss = 0.43 (1478.7 examples/sec; 0.087 sec/batch)
2017-05-02 18:35:59.257812: step 6000, loss = 0.39 (1490.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:36:19.932405: step 6100, loss = 0.63 (1448.9 examples/sec; 0.088 sec/batch)
2017-05-02 18:36:37.539974: step 6200, loss = 0.46 (1463.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:36:55.027342: step 6300, loss = 1.11 (1513.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:37:12.620446: step 6400, loss = 0.48 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 18:37:30.256258: step 6500, loss = 0.43 (1466.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:37:48.481925: step 6600, loss = 0.42 (1446.8 examples/sec; 0.088 sec/batch)
2017-05-02 18:38:06.351433: step 6700, loss = 0.41 (1486.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:38:24.185703: step 6800, loss = 0.54 (1462.1 examples/sec; 0.088 sec/batch)
2017-05-02 18:38:41.892307: step 6900, loss = 0.42 (1468.0 examples/sec; 0.087 sec/batch)
2017-05-02 18:38:59.628980: step 7000, loss = 0.46 (1496.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:39:18.859909: step 7100, loss = 0.41 (1513.6 examples/sec; 0.085 sec/batch)
2017-05-02 18:39:36.311547: step 7200, loss = 0.34 (1480.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:39:53.808609: step 7300, loss = 0.40 (1490.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:40:11.235237: step 7400, loss = 0.34 (1492.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:40:29.179641: step 7500, loss = 0.33 (1473.0 examples/sec; 0.087 sec/batch)
2017-05-02 18:40:46.849693: step 7600, loss = 0.46 (1470.3 examples/sec; 0.087 sec/batch)
2017-05-02 18:41:05.014433: step 7700, loss = 0.47 (1451.6 examples/sec; 0.088 sec/batch)
2017-05-02 18:41:22.665599: step 7800, loss = 0.35 (1453.1 examples/sec; 0.088 sec/batch)
2017-05-02 18:41:40.239491: step 7900, loss = 1.75 (1415.8 examples/sec; 0.090 sec/batch)
2017-05-02 18:41:57.873170: step 8000, loss = 0.44 (1469.7 examples/sec; 0.087 sec/batch)
2017-05-02 18:42:19.276744: step 8100, loss = 0.39 (1437.0 examples/sec; 0.089 sec/batch)
2017-05-02 18:42:37.163847: step 8200, loss = 0.42 (1454.6 examples/sec; 0.088 sec/batch)
2017-05-02 18:42:55.021150: step 8300, loss = 0.41 (1488.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:43:12.204493: step 8400, loss = 0.46 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-02 18:43:29.514160: step 8500, loss = 0.40 (1527.3 examples/sec; 0.084 sec/batch)
2017-05-02 18:43:47.282228: step 8600, loss = 0.46 (1441.1 examples/sec; 0.089 sec/batch)
2017-05-02 18:44:04.644396: step 8700, loss = 0.44 (1499.9 examples/sec; 0.085 sec/batch)
2017-05-02 18:44:22.103160: step 8800, loss = 0.43 (1509.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:44:39.678403: step 8900, loss = 0.40 (1476.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:44:57.180880: step 9000, loss = 0.43 (1464.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:45:16.103237: step 9100, loss = 0.40 (1465.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:45:33.694017: step 9200, loss = 0.38 (1520.8 examples/sec; 0.084 sec/batch)
2017-05-02 18:45:51.112190: step 9300, loss = 0.38 (1480.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:46:08.605784: step 9400, loss = 0.35 (1498.5 examples/sec; 0.085 sec/batch)
2017-05-02 18:46:26.219260: step 9500, loss = 0.43 (1423.1 examples/sec; 0.090 sec/batch)
2017-05-02 18:46:43.814697: step 9600, loss = 0.33 (1431.4 examples/sec; 0.089 sec/batch)
2017-05-02 18:47:01.252524: step 9700, loss = 0.37 (1482.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:47:18.921002: step 9800, loss = 0.35 (1389.1 examples/sec; 0.092 sec/batch)
2017-05-02 18:47:36.361758: step 9900, loss = 0.37 (1473.4 examples/sec; 0.087 sec/batch)
2017-05-02 18:47:53.957981: step 10000, loss = 0.59 (1468.6 examples/sec; 0.087 sec/batch)
2017-05-02 18:48:13.168870: step 10100, loss = 0.39 (1492.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:48:30.775244: step 10200, loss = 0.35 (1503.4 examples/sec; 0.085 sec/batch)
2017-05-02 18:48:48.338551: step 10300, loss = 0.41 (1486.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:49:06.008107: step 10400, loss = 0.37 (1469.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:49:23.472385: step 10500, loss = 0.34 (1489.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:49:40.918897: step 10600, loss = 0.47 (1497.5 examples/sec; 0.085 sec/batch)
2017-05-02 18:49:58.334472: step 10700, loss = 0.37 (1519.7 examples/sec; 0.084 sec/batch)
2017-05-02 18:50:15.759204: step 10800, loss = 0.32 (1511.3 examples/sec; 0.085 sec/batch)
2017-05-02 18:50:33.226115: step 10900, loss = 0.43 (1480.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:50:51.024537: step 11000, loss = 0.39 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:51:10.162526: step 11100, loss = 0.37 (1483.3 examples/sec; 0.086 sec/batch)
2017-05-02 18:51:27.709181: step 11200, loss = 0.41 (1477.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:51:45.200273: step 11300, loss = 0.39 (1491.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:52:02.699287: step 11400, loss = 0.37 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 18:52:20.389737: step 11500, loss = 0.47 (1481.1 examples/sec; 0.086 sec/batch)
2017-05-02 18:52:38.020897: step 11600, loss = 0.42 (1518.0 examples/sec; 0.084 sec/batch)
2017-05-02 18:52:55.755700: step 11700, loss = 0.36 (1356.2 examples/sec; 0.094 sec/batch)
2017-05-02 18:53:13.345455: step 11800, loss = 0.33 (1498.2 examples/sec; 0.085 sec/batch)
2017-05-02 18:53:30.880150: step 11900, loss = 0.42 (1480.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:53:48.340382: step 12000, loss = 0.38 (1491.6 examples/sec; 0.086 sec/batch)
2017-05-02 18:54:07.494958: step 12100, loss = 0.32 (1457.7 examples/sec; 0.088 sec/batch)
2017-05-02 18:54:24.898706: step 12200, loss = 0.41 (1512.9 examples/sec; 0.085 sec/batch)
2017-05-02 18:54:42.518339: step 12300, loss = 0.36 (1483.2 examples/sec; 0.086 sec/batch)
2017-05-02 18:55:00.063674: step 12400, loss = 0.30 (1472.9 examples/sec; 0.087 sec/batch)
2017-05-02 18:55:17.535631: step 12500, loss = 0.35 (1473.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:55:35.128185: step 12600, loss = 0.34 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-02 18:55:52.546169: step 12700, loss = 0.53 (1495.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:56:10.137206: step 12800, loss = 0.41 (1511.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:56:27.714641: step 12900, loss = 0.70 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 18:56:45.345258: step 13000, loss = 0.43 (1463.2 examples/sec; 0.087 sec/batch)
2017-05-02 18:57:04.552542: step 13100, loss = 0.50 (1466.3 examples/sec; 0.087 sec/batch)
2017-05-02 18:57:22.283365: step 13200, loss = 0.35 (1484.0 examples/sec; 0.086 sec/batch)
2017-05-02 18:57:39.843609: step 13300, loss = 0.43 (1460.2 examples/sec; 0.088 sec/batch)
2017-05-02 18:57:57.546242: step 13400, loss = 0.42 (1417.6 examples/sec; 0.090 sec/batch)
2017-05-02 18:58:15.165322: step 13500, loss = 0.44 (1487.5 examples/sec; 0.086 sec/batch)
2017-05-02 18:58:32.706169: step 13600, loss = 0.41 (1497.7 examples/sec; 0.085 sec/batch)
2017-05-02 18:58:50.245692: step 13700, loss = 0.35 (1508.1 examples/sec; 0.085 sec/batch)
2017-05-02 18:59:07.719432: step 13800, loss = 0.41 (1492.7 examples/sec; 0.086 sec/batch)
2017-05-02 18:59:25.140024: step 13900, loss = 0.37 (1529.6 examples/sec; 0.084 sec/batch)
2017-05-02 18:59:42.951909: step 14000, loss = 0.36 (1461.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:00:02.209814: step 14100, loss = 0.42 (1461.9 examples/sec; 0.088 sec/batch)
2017-05-02 19:00:19.957586: step 14200, loss = 0.36 (1487.8 examples/sec; 0.086 sec/batch)
2017-05-02 19:00:37.779381: step 14300, loss = 0.48 (1502.7 examples/sec; 0.085 sec/batch)
2017-05-02 19:00:55.317083: step 14400, loss = 0.42 (1479.2 examples/sec; 0.087 sec/batch)
2017-05-02 19:01:12.807641: step 14500, loss = 0.31 (1462.5 examples/sec; 0.088 sec/batch)
2017-05-02 19:01:30.341507: step 14600, loss = 0.39 (1447.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:01:48.007340: step 14700, loss = 0.42 (1450.5 examples/sec; 0.088 sec/batch)
2017-05-02 19:02:05.796259: step 14800, loss = 0.37 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 19:02:23.404595: step 14900, loss = 0.30 (1492.5 examples/sec; 0.086 sec/batch)
2017-05-02 19:02:41.047996: step 15000, loss = 0.35 (1481.9 examples/sec; 0.086 sec/batch)
2017-05-02 19:03:02.008369: step 15100, loss = 0.35 (1499.4 examples/sec; 0.085 sec/batch)
2017-05-02 19:03:19.538124: step 15200, loss = 0.36 (1470.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:03:37.046255: step 15300, loss = 0.37 (1534.8 examples/sec; 0.083 sec/batch)
2017-05-02 19:03:54.932137: step 15400, loss = 0.37 (1426.7 examples/sec; 0.090 sec/batch)
2017-05-02 19:04:12.498391: step 15500, loss = 0.41 (1518.9 examples/sec; 0.084 sec/batch)
2017-05-02 19:04:30.001789: step 15600, loss = 0.49 (1461.4 examples/sec; 0.088 sec/batch)
2017-05-02 19:04:47.458093: step 15700, loss = 0.44 (1485.4 examples/sec; 0.086 sec/batch)
2017-05-02 19:05:04.953351: step 15800, loss = 0.34 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 19:05:22.449971: step 15900, loss = 0.35 (1476.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:05:40.013330: step 16000, loss = 0.30 (1502.1 examples/sec; 0.085 sec/batch)
2017-05-02 19:05:59.042067: step 16100, loss = 0.36 (1441.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:06:16.663428: step 16200, loss = 0.32 (1465.0 examples/sec; 0.087 sec/batch)
2017-05-02 19:06:34.994918: step 16300, loss = 0.58 (1501.6 examples/sec; 0.085 sec/batch)
2017-05-02 19:06:52.582933: step 16400, loss = 0.34 (1462.1 examples/sec; 0.088 sec/batch)
2017-05-02 19:07:10.345489: step 16500, loss = 0.46 (1476.1 examples/sec; 0.087 sec/batch)
2017-05-02 19:07:28.538790: step 16600, loss = 0.39 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:07:46.259003: step 16700, loss = 0.33 (1454.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:08:03.843245: step 16800, loss = 0.35 (1499.7 examples/sec; 0.085 sec/batch)
2017-05-02 19:08:21.582435: step 16900, loss = 0.41 (1385.7 examples/sec; 0.092 sec/batch)
2017-05-02 19:08:39.609675: step 17000, loss = 0.51 (1336.7 examples/sec; 0.096 sec/batch)
2017-05-02 19:08:59.686966: step 17100, loss = 0.33 (1425.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:09:18.109851: step 17200, loss = 0.34 (1455.9 examples/sec; 0.088 sec/batch)
2017-05-02 19:09:36.768066: step 17300, loss = 0.32 (1461.0 examples/sec; 0.088 sec/batch)
2017-05-02 19:09:54.993777: step 17400, loss = 0.31 (1429.3 examples/sec; 0.090 sec/batch)
2017-05-02 19:10:13.112570: step 17500, loss = 0.36 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 19:10:31.035481: step 17600, loss = 0.44 (1362.7 examples/sec; 0.094 sec/batch)
2017-05-02 19:10:49.445350: step 17700, loss = 0.29 (1452.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:11:07.164353: step 17800, loss = 0.30 (1503.7 examples/sec; 0.085 sec/batch)
2017-05-02 19:11:24.963356: step 17900, loss = 0.39 (1416.2 examples/sec; 0.090 sec/batch)
2017-05-02 19:11:43.033899: step 18000, loss = 0.34 (1424.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:12:02.383392: step 18100, loss = 0.35 (1499.8 examples/sec; 0.085 sec/batch)
2017-05-02 19:12:20.446696: step 18200, loss = 0.31 (1478.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:12:38.608421: step 18300, loss = 0.28 (1386.4 examples/sec; 0.092 sec/batch)
2017-05-02 19:12:56.499342: step 18400, loss = 0.38 (1458.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:13:14.464512: step 18500, loss = 0.35 (1412.2 examples/sec; 0.091 sec/batch)
2017-05-02 19:13:32.291663: step 18600, loss = 0.29 (1429.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:13:50.120186: step 18700, loss = 0.39 (1474.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:14:08.142327: step 18800, loss = 0.35 (1430.8 examples/sec; 0.089 sec/batch)
2017-05-02 19:14:26.365027: step 18900, loss = 0.32 (1416.7 examples/sec; 0.090 sec/batch)
2017-05-02 19:14:45.572758: step 19000, loss = 0.42 (1339.0 examples/sec; 0.096 sec/batch)
2017-05-02 19:15:05.424098: step 19100, loss = 0.43 (1362.7 examples/sec; 0.094 sec/batch)
2017-05-02 19:15:23.837732: step 19200, loss = 0.35 (1434.8 examples/sec; 0.089 sec/batch)
2017-05-02 19:15:42.500734: step 19300, loss = 0.38 (1341.4 examples/sec; 0.095 sec/batch)
2017-05-02 19:16:00.707296: step 19400, loss = 0.32 (1364.7 examples/sec; 0.094 sec/batch)
2017-05-02 19:16:19.518199: step 19500, loss = 0.36 (1391.7 examples/sec; 0.092 sec/batch)
2017-05-02 19:16:39.068137: step 19600, loss = 0.39 (1170.1 examples/sec; 0.109 sec/batch)
2017-05-02 19:16:56.897011: step 19700, loss = 0.35 (1453.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:17:15.674760: step 19800, loss = 0.35 (1454.1 examples/sec; 0.088 sec/batch)
2017-05-02 19:17:33.826030: step 19900, loss = 0.30 (1470.1 examples/sec; 0.087 sec/batch)
2017-05-02 19:17:52.196728: step 20000, loss = 0.38 (1363.8 examples/sec; 0.094 sec/batch)
2017-05-02 19:18:12.524483: step 20100, loss = 0.38 (1389.8 examples/sec; 0.092 sec/batch)
2017-05-02 19:18:30.734670: step 20200, loss = 0.30 (1446.2 examples/sec; 0.089 sec/batch)
2017-05-02 19:18:49.244318: step 20300, loss = 0.33 (1340.2 examples/sec; 0.096 sec/batch)
2017-05-02 19:19:08.377790: step 20400, loss = 0.35 (1327.9 examples/sec; 0.096 sec/batch)
2017-05-02 19:19:26.790488: step 20500, loss = 0.35 (1488.1 examples/sec; 0.086 sec/batch)
2017-05-02 19:19:45.237230: step 20600, loss = 0.29 (1417.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:20:03.878885: step 20700, loss = 0.32 (1394.9 examples/sec; 0.092 sec/batch)
2017-05-02 19:20:23.941247: step 20800, loss = 0.29 (1374.7 examples/sec; 0.093 sec/batch)
2017-05-02 19:20:43.356740: step 20900, loss = 0.36 (1526.0 examples/sec; 0.084 sec/batch)
2017-05-02 19:21:01.675827: step 21000, loss = 0.45 (1477.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:21:23.742398: step 21100, loss = 0.55 (1486.5 examples/sec; 0.086 sec/batch)
2017-05-02 19:21:42.449329: step 21200, loss = 0.57 (1399.9 examples/sec; 0.091 sec/batch)
2017-05-02 19:22:00.511527: step 21300, loss = 0.41 (1466.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:22:18.614471: step 21400, loss = 0.49 (1458.0 examples/sec; 0.088 sec/batch)
2017-05-02 19:22:39.361516: step 21500, loss = 0.50 (1455.4 examples/sec; 0.088 sec/batch)
2017-05-02 19:22:58.840931: step 21600, loss = 0.52 (1423.1 examples/sec; 0.090 sec/batch)
2017-05-02 19:23:18.064610: step 21700, loss = 0.41 (1455.2 examples/sec; 0.088 sec/batch)
2017-05-02 19:23:39.494070: step 21800, loss = 0.48 (3388.3 examples/sec; 0.038 sec/batch)
2017-05-02 19:23:58.993090: step 21900, loss = 0.41 (1438.9 examples/sec; 0.089 sec/batch)
2017-05-02 19:24:18.398755: step 22000, loss = 0.41 (1474.2 examples/sec; 0.087 sec/batch)
2017-05-02 19:24:38.792511: step 22100, loss = 0.47 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 19:24:58.967042: step 22200, loss = 0.42 (1437.4 examples/sec; 0.089 sec/batch)
2017-05-02 19:25:18.655158: step 22300, loss = 0.37 (1428.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:25:37.371493: step 22400, loss = 0.42 (1439.6 examples/sec; 0.089 sec/batch)
2017-05-02 19:25:56.138613: step 22500, loss = 0.45 (1425.2 examples/sec; 0.090 sec/batch)
2017-05-02 19:26:15.428069: step 22600, loss = 0.34 (1428.0 examples/sec; 0.090 sec/batch)
2017-05-02 19:26:35.268016: step 22700, loss = 0.40 (1471.1 examples/sec; 0.087 sec/batch)
2017-05-02 19:26:54.563911: step 22800, loss = 0.36 (1470.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:27:13.194422: step 22900, loss = 0.53 (1512.5 examples/sec; 0.085 sec/batch)
2017-05-02 19:27:33.999811: step 23000, loss = 0.37 (1451.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:27:56.804232: step 23100, loss = 0.44 (2730.9 examples/sec; 0.047 sec/batch)
2017-05-02 19:28:16.515597: step 23200, loss = 0.36 (1379.7 examples/sec; 0.093 sec/batch)
2017-05-02 19:28:36.943751: step 23300, loss = 0.39 (1473.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:28:56.408942: step 23400, loss = 0.44 (1302.8 examples/sec; 0.098 sec/batch)
2017-05-02 19:29:14.876579: step 23500, loss = 0.34 (1388.4 examples/sec; 0.092 sec/batch)
2017-05-02 19:29:33.806582: step 23600, loss = 0.36 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 19:29:52.324032: step 23700, loss = 0.34 (1783.9 examples/sec; 0.072 sec/batch)
2017-05-02 19:30:10.968760: step 23800, loss = 0.37 (1515.0 examples/sec; 0.084 sec/batch)
2017-05-02 19:30:29.064489: step 23900, loss = 0.36 (1404.6 examples/sec; 0.091 sec/batch)
2017-05-02 19:30:47.335530: step 24000, loss = 0.33 (1471.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:31:07.387064: step 24100, loss = 0.35 (1500.5 examples/sec; 0.085 sec/batch)
2017-05-02 19:31:26.352003: step 24200, loss = 0.35 (1448.9 examples/sec; 0.088 sec/batch)
2017-05-02 19:31:44.566633: step 24300, loss = 0.40 (1467.6 examples/sec; 0.087 sec/batch)
2017-05-02 19:32:03.042462: step 24400, loss = 0.31 (1464.3 examples/sec; 0.087 sec/batch)
2017-05-02 19:32:21.053969: step 24500, loss = 0.45 (1509.4 examples/sec; 0.085 sec/batch)
2017-05-02 19:32:39.303010: step 24600, loss = 0.33 (1452.5 examples/sec; 0.088 sec/batch)
2017-05-02 19:32:57.239311: step 24700, loss = 0.30 (1466.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:33:15.257511: step 24800, loss = 0.52 (1496.2 examples/sec; 0.086 sec/batch)
2017-05-02 19:33:33.179198: step 24900, loss = 0.39 (1399.8 examples/sec; 0.091 sec/batch)
2017-05-02 19:33:50.923114: step 25000, loss = 0.39 (1476.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:34:10.406094: step 25100, loss = 0.36 (1488.8 examples/sec; 0.086 sec/batch)
2017-05-02 19:34:29.471843: step 25200, loss = 0.37 (1443.1 examples/sec; 0.089 sec/batch)
2017-05-02 19:34:47.716232: step 25300, loss = 0.31 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-02 19:35:06.617739: step 25400, loss = 0.38 (1361.6 examples/sec; 0.094 sec/batch)
2017-05-02 19:35:25.294312: step 25500, loss = 0.32 (1456.6 examples/sec; 0.088 sec/batch)
2017-05-02 19:35:44.472570: step 25600, loss = 0.36 (1494.5 examples/sec; 0.086 sec/batch)
2017-05-02 19:36:02.596812: step 25700, loss = 0.37 (1484.9 examples/sec; 0.086 sec/batch)
2017-05-02 19:36:20.922565: step 25800, loss = 0.35 (1463.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:36:39.107621: step 25900, loss = 0.36 (1479.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:36:58.040363: step 26000, loss = 0.35 (1434.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:37:18.175585: step 26100, loss = 0.39 (1410.2 examples/sec; 0.091 sec/batch)
2017-05-02 19:37:36.565564: step 26200, loss = 0.34 (1429.2 examples/sec; 0.090 sec/batch)
2017-05-02 19:37:55.132365: step 26300, loss = 0.32 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 19:38:13.263245: step 26400, loss = 0.39 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-02 19:38:31.294745: step 26500, loss = 0.28 (1350.5 examples/sec; 0.095 sec/batch)
2017-05-02 19:38:50.471828: step 26600, loss = 0.35 (1477.4 examples/sec; 0.087 sec/batch)
2017-05-02 19:39:09.409920: step 26700, loss = 0.29 (1440.0 examples/sec; 0.089 sec/batch)
2017-05-02 19:39:27.805426: step 26800, loss = 0.35 (1463.7 examples/sec; 0.087 sec/batch)
2017-05-02 19:39:46.368611: step 26900, loss = 0.31 (1451.6 examples/sec; 0.088 sec/batch)
2017-05-02 19:40:04.373904: step 27000, loss = 0.31 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 19:40:23.941890: step 27100, loss = 0.41 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 19:40:42.649431: step 27200, loss = 0.36 (1709.5 examples/sec; 0.075 sec/batch)
2017-05-02 19:41:00.727540: step 27300, loss = 0.32 (1436.0 examples/sec; 0.089 sec/batch)
2017-05-02 19:41:18.954181: step 27400, loss = 0.29 (1430.6 examples/sec; 0.089 sec/batch)
2017-05-02 19:41:37.557101: step 27500, loss = 0.34 (601.7 examples/sec; 0.213 sec/batch)
2017-05-02 19:41:55.594021: step 27600, loss = 0.43 (1412.4 examples/sec; 0.091 sec/batch)
2017-05-02 19:42:14.221796: step 27700, loss = 0.36 (1441.5 examples/sec; 0.089 sec/batch)
2017-05-02 19:42:32.411910: step 27800, loss = 0.35 (1405.2 examples/sec; 0.091 sec/batch)
2017-05-02 19:42:50.486484: step 27900, loss = 0.35 (1487.7 examples/sec; 0.086 sec/batch)
2017-05-02 19:43:08.803950: step 28000, loss = 0.37 (1454.8 examples/sec; 0.088 sec/batch)
2017-05-02 19:43:29.006027: step 28100, loss = 0.40 (1417.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:43:47.729508: step 28200, loss = 0.32 (1433.1 examples/sec; 0.089 sec/batch)
2017-05-02 19:44:06.498220: step 28300, loss = 0.31 (1441.5 examples/sec; 0.089 sec/batch)
2017-05-02 19:44:24.973333: step 28400, loss = 0.35 (1471.0 examples/sec; 0.087 sec/batch)
2017-05-02 19:44:44.209580: step 28500, loss = 0.33 (1448.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:45:02.517580: step 28600, loss = 0.32 (1415.1 examples/sec; 0.090 sec/batch)
2017-05-02 19:45:22.081939: step 28700, loss = 0.30 (1428.1 examples/sec; 0.090 sec/batch)
2017-05-02 19:45:40.661183: step 28800, loss = 0.28 (1429.3 examples/sec; 0.090 sec/batch)
2017-05-02 19:45:59.584009: step 28900, loss = 0.28 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 19:46:17.758723: step 29000, loss = 0.31 (1465.0 examples/sec; 0.087 sec/batch)
2017-05-02 19:46:37.360546: step 29100, loss = 0.33 (1515.3 examples/sec; 0.084 sec/batch)
2017-05-02 19:46:56.190999: step 29200, loss = 0.35 (1466.8 examples/sec; 0.087 sec/batch)
2017-05-02 19:47:14.485961: step 29300, loss = 0.51 (1405.1 examples/sec; 0.091 sec/batch)
2017-05-02 19:47:33.094404: step 29400, loss = 0.36 (1450.3 examples/sec; 0.088 sec/batch)
2017-05-02 19:47:52.732181: step 29500, loss = 0.45 (1448.6 examples/sec; 0.088 sec/batch)
2017-05-02 19:48:11.134463: step 29600, loss = 0.36 (1425.5 examples/sec; 0.090 sec/batch)
2017-05-02 19:48:29.940185: step 29700, loss = 0.35 (1417.4 examples/sec; 0.090 sec/batch)
2017-05-02 19:48:49.019701: step 29800, loss = 0.32 (1462.7 examples/sec; 0.088 sec/batch)
2017-05-02 19:49:07.870342: step 29900, loss = 0.35 (1487.0 examples/sec; 0.086 sec/batch)
2017-05-02 19:49:29.605143: precision @ 1 = 0.828
2017-05-02 19:49:30.002464: DONE
########################################################################################
########################################################################################
########################################################################################
Summary of Network 1:
conv1Shape: [2, 2, 3, 4]
pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME
conv2Shape: [2, 2, 4, 10]
pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME
local3InputDepth: 640  | local3OutputDepth: 640
local4InputDepth: 640  | local4OutputDepth: 64
softmax_linearInput: 64

Number of hidden parameters of conv1: 48
Number of hidden parameters of conv1Biases: 4
Number of hidden parameters of conv2: 160
Number of hidden parameters of conv2Biases: 10
Number of hidden parameters of local3: 409600
Number of hidden parameters of local3Biases: 640
Number of hidden parameters of local4: 40960
Number of hidden parameters of local4Biases: 64
Number of hidden parameters of softmax: 640
Number of hidden parameters of softmaxBiases: 10
Total number of hidden parameters: 452136
########################################################################################
########################################################################################
########################################################################################
2017-05-06 23:41:08.413316: Running on MSI...
The experiment details:
network = 1 max_steps = 100 log_frequency = 100 num_gpus = 2
