{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tfFLAGS\n",
    "tfFLAGS.LEARNING_RATE_DECAY_FACTOR = 0.1;           tfFLAGS.NUM_CLASSES = 10;       tfFLAGS.run_once = True\n",
    "tfFLAGS.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000;   tfFLAGS.batch_size = 128;       tfFLAGS.use_fp16 = False\n",
    "tfFLAGS.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000;    tfFLAGS.max_steps = 4000;      tfFLAGS.log_frequency = 100\n",
    "tfFLAGS.MOVING_AVERAGE_DECAY = 0.9999;   tfFLAGS.NUM_EPOCHS_PER_DECAY = 350.0;      tfFLAGS.IMAGE_SIZE = 32;\n",
    "tfFLAGS.INITIAL_LEARNING_RATE = 0.1;     tfFLAGS.eval_interval_secs = 60 * 5;       tfFLAGS.num_examples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1Size = 3; conv1Shape = [conv1Size, conv1Size, 3, 64]\n",
    "conv2Size = 5; conv2Shape = [conv2Size, conv2Size, 64, 128]\n",
    "\n",
    "pool1S = 2; pool1ksize=[1, pool1S, pool1S, 1]; pool1St = 2; pool1strides=[1, pool1St, pool1St, 1]; pool1padding='SAME'\n",
    "pool2S = 2; pool2ksize=[1, pool2S, pool2S, 1]; pool2St = 2; pool2strides=[1, pool2St, pool2St, 1]; pool2padding='SAME'\n",
    "\n",
    "local3InputDepth = 384; local3OutputDepth = 384\n",
    "local4InputDepth = local3OutputDepth; local4OutputDepth = 192\n",
    "softmax_linearInput = local4OutputDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network summary:\n",
      "conv1Shape: [3, 3, 3, 64]\n",
      "pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME\n",
      "conv2Shape: [5, 5, 64, 128]\n",
      "pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME\n",
      "local3InputDepth: 384  | local3OutputDepth: 384\n",
      "local4InputDepth: 384  | local4OutputDepth: 192\n",
      "softmax_linearInput: 192\n",
      "\n",
      "Number of hidden parameters of conv1: 1792\n",
      "Number of hidden parameters of conv2: 204928\n",
      "Number of hidden parameters of local3: 147840\n",
      "Number of hidden parameters of local4: 73920\n",
      "Number of hidden parameters of softmax: 1930\n",
      "Total number of hidden parameters: 430410\n"
     ]
    }
   ],
   "source": [
    "print_('Network summary:')\n",
    "print_('conv1Shape:', conv1Shape)\n",
    "print_('pool1ksize:', pool1ksize, ' | pool1strides:', pool1strides, ' | pool1padding:', pool1padding)\n",
    "print_('conv2Shape:', conv2Shape)\n",
    "print_('pool2ksize:', pool2ksize, ' | pool2strides:', pool2strides, ' | pool2padding:', pool2padding)\n",
    "print_('local3InputDepth:', local3InputDepth, ' | local3OutputDepth:', local3OutputDepth)\n",
    "print_('local4InputDepth:', local4InputDepth, ' | local4OutputDepth:', local4OutputDepth)\n",
    "print_('softmax_linearInput:', softmax_linearInput)\n",
    "print_()\n",
    "numParamConv1 = (conv1Shape[0] * conv1Shape[1] * conv1Shape[2] + 1) * conv1Shape[-1]\n",
    "print_('Number of hidden parameters of conv1:', numParamConv1)\n",
    "#print_('Number of hidden parameters of norm1:', numParamConv1 / pool1S**1)\n",
    "numParamConv2 = (conv2Shape[0] * conv2Shape[1] * conv2Shape[2] + 1) * conv2Shape[-1]\n",
    "print_('Number of hidden parameters of conv2:', numParamConv2)\n",
    "numParamLocal3 = (local3InputDepth + 1) * local3OutputDepth\n",
    "print_('Number of hidden parameters of local3:', numParamLocal3)\n",
    "numParamLocal4 = (local4InputDepth + 1) * local4OutputDepth\n",
    "print_('Number of hidden parameters of local4:', numParamLocal4)\n",
    "numParamsoftmax = (softmax_linearInput + 1) * tfFLAGS.NUM_CLASSES\n",
    "print_('Number of hidden parameters of softmax:', numParamsoftmax)\n",
    "print_('Total number of hidden parameters:', numParamConv1 + numParamConv2 + numParamLocal3 + numParamLocal4 + numParamsoftmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MyModel import * \n",
    "def inferenceOnJupyter(images):\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', shape=conv1Shape, stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu('biases', [conv1Shape[-1]], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        activation_summary(conv1)\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=pool1ksize, strides=pool1strides, padding=pool1padding, name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', shape=conv2Shape, stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu('biases', [conv2Shape[-1]], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        activation_summary(conv2)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=pool2ksize, strides=pool2strides, padding=pool2padding, name='pool2')\n",
    "\n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        reshape = tf.reshape(pool2, [tfFLAGS.batch_size, -1])\n",
    "        local3InputDepth = reshape.get_shape()[1].value\n",
    "        weights = variable_with_weight_decay('weights', shape=[local3InputDepth, local3OutputDepth], stddev=0.04, wd=0.004)\n",
    "        biases = variable_on_cpu('biases', [local3OutputDepth], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)        \n",
    "        activation_summary(local3)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = variable_with_weight_decay('weights', shape=[local3OutputDepth, local4OutputDepth], stddev=0.04, wd=0.004)\n",
    "        biases = variable_on_cpu('biases', [local4OutputDepth], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        activation_summary(local4)\n",
    "        \n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = variable_with_weight_decay('weights', [softmax_linearInput, tfFLAGS.NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "        biases = variable_on_cpu('biases', [tfFLAGS.NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        activation_summary(softmax_linear)\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MyModel\n",
    "inferenceOnMyModel = MyModel.inference\n",
    "MyModel.inference = inferenceOnJupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "########################################################################################\n",
      "########################################################################################\n",
      "2017-05-06 05:20:37.629658: Running on server...\n",
      "The experiment details:\n",
      "max_steps = 4000 log_frequency = 100 num_gpus = 2\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "2017-05-06 05:21:03.312783: step 0, loss = 10.28 (12.5 examples/sec; 10.261 sec/batch)\n",
      "2017-05-06 05:21:24.767219: step 100, loss = 9.07 (1279.0 examples/sec; 0.100 sec/batch)\n",
      "2017-05-06 05:21:44.548738: step 200, loss = 8.50 (1316.2 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:22:05.408034: step 300, loss = 7.86 (1318.2 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:22:25.192726: step 400, loss = 7.34 (1322.5 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:22:44.894839: step 500, loss = 6.44 (1322.3 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:23:04.652071: step 600, loss = 6.19 (1326.4 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:23:24.389363: step 700, loss = 5.56 (1222.5 examples/sec; 0.105 sec/batch)\n",
      "2017-05-06 05:23:47.087676: step 800, loss = 5.45 (1327.4 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:24:06.829623: step 900, loss = 4.92 (1262.2 examples/sec; 0.101 sec/batch)\n",
      "2017-05-06 05:24:26.558771: step 1000, loss = 4.58 (1321.9 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:24:47.511441: step 1100, loss = 4.38 (1319.5 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:25:07.255013: step 1200, loss = 4.22 (1296.0 examples/sec; 0.099 sec/batch)\n",
      "2017-05-06 05:25:31.948879: step 1300, loss = 3.73 (1319.2 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:25:53.349751: step 1400, loss = 3.36 (1361.9 examples/sec; 0.094 sec/batch)\n",
      "2017-05-06 05:26:15.883314: step 1500, loss = 3.14 (1328.3 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:26:39.914647: step 1600, loss = 3.08 (1337.8 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:26:59.689280: step 1700, loss = 2.92 (1325.3 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:27:19.405027: step 1800, loss = 2.77 (1307.5 examples/sec; 0.098 sec/batch)\n",
      "2017-05-06 05:27:39.151103: step 1900, loss = 2.51 (1328.5 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:27:58.900511: step 2000, loss = 2.29 (1330.4 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:28:20.443044: step 2100, loss = 2.16 (1298.2 examples/sec; 0.099 sec/batch)\n",
      "2017-05-06 05:28:42.609356: step 2200, loss = 2.06 (1328.1 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:29:03.348807: step 2300, loss = 1.87 (223.1 examples/sec; 0.574 sec/batch)\n",
      "2017-05-06 05:29:26.126846: step 2400, loss = 1.93 (1295.8 examples/sec; 0.099 sec/batch)\n",
      "2017-05-06 05:29:45.911712: step 2500, loss = 1.80 (1269.1 examples/sec; 0.101 sec/batch)\n",
      "2017-05-06 05:30:15.865749: step 2600, loss = 1.57 (1319.9 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:30:50.866291: step 2700, loss = 1.53 (1132.0 examples/sec; 0.113 sec/batch)\n",
      "2017-05-06 05:31:13.085583: step 2800, loss = 1.37 (1309.8 examples/sec; 0.098 sec/batch)\n",
      "2017-05-06 05:31:32.839624: step 2900, loss = 1.27 (1326.6 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:31:52.567144: step 3000, loss = 1.35 (1333.0 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:32:13.821877: step 3100, loss = 1.32 (1333.4 examples/sec; 0.096 sec/batch)\n",
      "2017-05-06 05:32:33.595946: step 3200, loss = 1.32 (1318.2 examples/sec; 0.097 sec/batch)\n",
      "2017-05-06 05:32:53.391983: step 3300, loss = 1.15 (1305.3 examples/sec; 0.098 sec/batch)\n",
      "2017-05-06 05:33:13.217718: step 3400, loss = 1.08 (1287.7 examples/sec; 0.099 sec/batch)\n",
      "2017-05-06 05:33:33.012048: step 3500, loss = 1.08 (1311.8 examples/sec; 0.098 sec/batch)\n",
      "2017-05-06 05:33:52.805668: step 3600, loss = 1.00 (1308.1 examples/sec; 0.098 sec/batch)\n",
      "2017-05-06 05:34:12.589351: step 3700, loss = 0.92 (1305.5 examples/sec; 0.098 sec/batch)\n",
      "2017-05-06 05:34:32.413867: step 3800, loss = 0.84 (1395.1 examples/sec; 0.092 sec/batch)\n",
      "2017-05-06 05:34:52.189158: step 3900, loss = 1.05 (1321.3 examples/sec; 0.097 sec/batch)\n",
      "Evaluation results:\n",
      "2017-05-06 05:35:15.308766: Total Predictions = 1024\n",
      "2017-05-06 05:35:15.315709: Correct Predictions = 821\n",
      "2017-05-06 05:35:15.322752: Wrong Predictions = 203\n",
      "2017-05-06 05:35:15.328866: precision @ 1 = 0.802\n",
      "2017-05-06 05:35:15.800302: DONE\n",
      "########################################################################################\n",
      "########################################################################################\n",
      "########################################################################################\n"
     ]
    }
   ],
   "source": [
    "from runCIFAR10_Tensorflow import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
