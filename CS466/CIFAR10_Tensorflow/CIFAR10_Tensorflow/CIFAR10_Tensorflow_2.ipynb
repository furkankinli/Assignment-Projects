{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tfFLAGS\n",
    "tfFLAGS.LEARNING_RATE_DECAY_FACTOR = 0.1;           tfFLAGS.NUM_CLASSES = 10;       tfFLAGS.run_once = True\n",
    "tfFLAGS.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000;   tfFLAGS.batch_size = 128;       tfFLAGS.use_fp16 = False\n",
    "tfFLAGS.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000;    tfFLAGS.max_steps = 4000;      tfFLAGS.log_frequency = 100\n",
    "tfFLAGS.MOVING_AVERAGE_DECAY = 0.9999;   tfFLAGS.NUM_EPOCHS_PER_DECAY = 350.0;      tfFLAGS.IMAGE_SIZE = 32;\n",
    "tfFLAGS.INITIAL_LEARNING_RATE = 0.1;     tfFLAGS.eval_interval_secs = 60 * 5;       tfFLAGS.num_examples = 1000\n",
    "num_gpus = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1Size = 2; conv1Out = 4; conv1Shape = [conv1Size, conv1Size, 3, conv1Out]\n",
    "conv2Size = 2; conv2Shape = [conv2Size, conv2Size, conv1Out, 10]\n",
    "\n",
    "pool1S = 2; pool1ksize=[1, pool1S, pool1S, 1]; pool1St = 2; pool1strides=[1, pool1St, pool1St, 1]; pool1padding='SAME'\n",
    "pool2S = 2; pool2ksize=[1, pool2S, pool2S, 1]; pool2St = 2; pool2strides=[1, pool2St, pool2St, 1]; pool2padding='SAME'\n",
    "\n",
    "local3InputDepth = 64 * conv2Shape[-1]; local3OutputDepth = local3InputDepth\n",
    "local4InputDepth = local3OutputDepth; local4OutputDepth = 64\n",
    "softmax_linearInput = local4OutputDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MyModel import * \n",
    "def inferenceOnJupyter(images):\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', shape=conv1Shape, stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu('biases', [conv1Shape[-1]], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        activation_summary(conv1)\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=pool1ksize, strides=pool1strides, padding=pool1padding, name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', shape=conv2Shape, stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu('biases', [conv2Shape[-1]], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        activation_summary(conv2)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=pool2ksize, strides=pool2strides, padding=pool2padding, name='pool2')\n",
    "\n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        reshape = tf.reshape(pool2, [tfFLAGS.batch_size, -1])\n",
    "        local3InputDepth = reshape.get_shape()[1].value\n",
    "        print_('local3InputDepth:', local3InputDepth, ' | local3OutputDepth:', local3OutputDepth, reshape.get_shape())\n",
    "        weights = variable_with_weight_decay('weights', shape=[local3InputDepth, local3OutputDepth], stddev=0.04, wd=0.004)\n",
    "        biases = variable_on_cpu('biases', [local3OutputDepth], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)        \n",
    "        activation_summary(local3)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = variable_with_weight_decay('weights', shape=[local3OutputDepth, local4OutputDepth], stddev=0.04, wd=0.004)\n",
    "        biases = variable_on_cpu('biases', [local4OutputDepth], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        activation_summary(local4)\n",
    "        \n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = variable_with_weight_decay('weights', [softmax_linearInput, tfFLAGS.NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "        biases = variable_on_cpu('biases', [tfFLAGS.NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        activation_summary(softmax_linear)\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network summary:\n",
      "conv1Shape: [2, 2, 3, 4]\n",
      "pool1ksize: [1, 2, 2, 1]  | pool1strides: [1, 2, 2, 1]  | pool1padding: SAME\n",
      "conv2Shape: [2, 2, 4, 10]\n",
      "pool2ksize: [1, 2, 2, 1]  | pool2strides: [1, 2, 2, 1]  | pool2padding: SAME\n",
      "local3InputDepth: 640  | local3OutputDepth: 640\n",
      "local4InputDepth: 640  | local4OutputDepth: 64\n",
      "softmax_linearInput: 64\n",
      "\n",
      "Number of hidden parameters of conv1: 48\n",
      "Number of hidden parameters of conv1Biases: 4\n",
      "Number of hidden parameters of conv2: 160\n",
      "Number of hidden parameters of conv2Biases: 10\n",
      "Number of hidden parameters of local3: 409600\n",
      "Number of hidden parameters of local3Biases: 640\n",
      "Number of hidden parameters of local4: 40960\n",
      "Number of hidden parameters of local4Biases: 64\n",
      "Number of hidden parameters of softmax: 640\n",
      "Number of hidden parameters of softmaxBiases: 10\n",
      "Total number of hidden parameters: 452136\n"
     ]
    }
   ],
   "source": [
    "from PythonVersionHandler import print_\n",
    "print_('Network summary:')\n",
    "print_('conv1Shape:', conv1Shape)\n",
    "print_('pool1ksize:', pool1ksize, ' | pool1strides:', pool1strides, ' | pool1padding:', pool1padding)\n",
    "print_('conv2Shape:', conv2Shape)\n",
    "print_('pool2ksize:', pool2ksize, ' | pool2strides:', pool2strides, ' | pool2padding:', pool2padding)\n",
    "print_('local3InputDepth:', local3InputDepth, ' | local3OutputDepth:', local3OutputDepth)\n",
    "print_('local4InputDepth:', local4InputDepth, ' | local4OutputDepth:', local4OutputDepth)\n",
    "print_('softmax_linearInput:', softmax_linearInput)\n",
    "print_()\n",
    "numParamConv1 = (conv1Shape[0] * conv1Shape[1] * conv1Shape[2]) * conv1Shape[-1]\n",
    "print_('Number of hidden parameters of conv1:', numParamConv1)\n",
    "print_('Number of hidden parameters of conv1Biases:', conv1Shape[-1]) \n",
    "numParamConv1 += conv1Shape[-1]\n",
    "#print_('Number of hidden parameters of norm1:', numParamConv1 / pool1S**1)\n",
    "numParamConv2 = (conv2Shape[0] * conv2Shape[1] * conv2Shape[2]) * conv2Shape[-1]\n",
    "print_('Number of hidden parameters of conv2:', numParamConv2)\n",
    "print_('Number of hidden parameters of conv2Biases:', conv2Shape[-1]) \n",
    "numParamConv2 += conv2Shape[-1]\n",
    "numParamLocal3 = (local3InputDepth) * local3OutputDepth\n",
    "print_('Number of hidden parameters of local3:', numParamLocal3)\n",
    "print_('Number of hidden parameters of local3Biases:', local3OutputDepth)\n",
    "numParamLocal3 += local3OutputDepth\n",
    "numParamLocal4 = (local4InputDepth) * local4OutputDepth\n",
    "print_('Number of hidden parameters of local4:', numParamLocal4)\n",
    "print_('Number of hidden parameters of local4Biases:', local4OutputDepth)\n",
    "numParamLocal4 += local4OutputDepth\n",
    "numParamsoftmax = (softmax_linearInput) * tfFLAGS.NUM_CLASSES\n",
    "print_('Number of hidden parameters of softmax:', numParamsoftmax)\n",
    "print_('Number of hidden parameters of softmaxBiases:', tfFLAGS.NUM_CLASSES)\n",
    "numParamsoftmax += tfFLAGS.NUM_CLASSES\n",
    "print_('Total number of hidden parameters:', numParamConv1 + numParamConv2 + numParamLocal3 + numParamLocal4 + numParamsoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MyModel\n",
    "inferenceOnMyModel = MyModel.inference\n",
    "MyModel.inference = inferenceOnJupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "########################################################################################\n",
      "########################################################################################\n",
      "2017-05-06 07:36:16.559719: Running on server...\n",
      "The experiment details:\n",
      "max_steps = 4000 log_frequency = 100 num_gpus = 2\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)\n",
      "48\n",
      "4\n",
      "160\n",
      "10\n",
      "409600\n",
      "640\n",
      "40960\n",
      "64\n",
      "640\n",
      "10\n",
      "total_parameters 452136\n",
      "2017-05-06 07:36:38.020791: step 0, loss = 3.42 (16.3 examples/sec; 7.838 sec/batch)\n",
      "2017-05-06 07:36:50.364378: step 100, loss = 3.33 (2326.5 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:37:01.486676: step 200, loss = 3.25 (2452.4 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:37:12.617229: step 300, loss = 3.11 (2355.5 examples/sec; 0.054 sec/batch)\n",
      "2017-05-06 07:37:23.756607: step 400, loss = 2.77 (2364.8 examples/sec; 0.054 sec/batch)\n",
      "2017-05-06 07:37:34.923864: step 500, loss = 2.56 (2453.1 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:37:46.072453: step 600, loss = 2.49 (2430.3 examples/sec; 0.053 sec/batch)\n",
      "2017-05-06 07:37:57.158197: step 700, loss = 2.24 (2425.0 examples/sec; 0.053 sec/batch)\n",
      "2017-05-06 07:38:08.237591: step 800, loss = 2.18 (2320.0 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:38:19.353378: step 900, loss = 2.19 (2218.0 examples/sec; 0.058 sec/batch)\n",
      "2017-05-06 07:38:30.471288: step 1000, loss = 2.08 (2383.4 examples/sec; 0.054 sec/batch)\n",
      "2017-05-06 07:38:43.233239: step 1100, loss = 1.84 (2279.8 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:38:54.365851: step 1200, loss = 1.86 (2483.6 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:39:06.817261: step 1300, loss = 1.68 (2327.6 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:39:20.007390: step 1400, loss = 1.98 (2347.9 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:39:31.196121: step 1500, loss = 1.74 (2270.2 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:39:42.372333: step 1600, loss = 1.82 (2332.9 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:39:54.188438: step 1700, loss = 1.64 (2320.9 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:40:05.396618: step 1800, loss = 1.59 (2298.7 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:40:16.631801: step 1900, loss = 1.57 (2451.5 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:40:27.782265: step 2000, loss = 1.76 (2508.9 examples/sec; 0.051 sec/batch)\n",
      "2017-05-06 07:40:39.927714: step 2100, loss = 1.55 (2300.0 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:40:51.110175: step 2200, loss = 1.41 (2238.7 examples/sec; 0.057 sec/batch)\n",
      "2017-05-06 07:41:02.235215: step 2300, loss = 1.45 (2308.4 examples/sec; 0.055 sec/batch)\n",
      "2017-05-06 07:41:13.362725: step 2400, loss = 1.36 (2455.9 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:41:24.518605: step 2500, loss = 1.10 (2404.4 examples/sec; 0.053 sec/batch)\n",
      "2017-05-06 07:41:35.667900: step 2600, loss = 1.39 (2227.2 examples/sec; 0.057 sec/batch)\n",
      "2017-05-06 07:41:46.856220: step 2700, loss = 1.41 (2367.0 examples/sec; 0.054 sec/batch)\n",
      "2017-05-06 07:41:58.075557: step 2800, loss = 1.42 (2294.0 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:42:09.289064: step 2900, loss = 1.53 (2293.2 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:42:20.454109: step 3000, loss = 1.10 (2450.0 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:42:32.648030: step 3100, loss = 1.24 (2355.0 examples/sec; 0.054 sec/batch)\n",
      "2017-05-06 07:42:43.866135: step 3200, loss = 1.16 (2278.9 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:42:55.110855: step 3300, loss = 1.26 (2298.0 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:43:06.325069: step 3400, loss = 1.11 (2397.6 examples/sec; 0.053 sec/batch)\n",
      "2017-05-06 07:43:17.515225: step 3500, loss = 1.23 (2280.7 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:43:28.688504: step 3600, loss = 1.11 (2457.1 examples/sec; 0.052 sec/batch)\n",
      "2017-05-06 07:43:39.875295: step 3700, loss = 1.05 (2352.1 examples/sec; 0.054 sec/batch)\n",
      "2017-05-06 07:43:51.051435: step 3800, loss = 1.07 (2274.9 examples/sec; 0.056 sec/batch)\n",
      "2017-05-06 07:44:02.265056: step 3900, loss = 1.17 (2270.9 examples/sec; 0.056 sec/batch)\n",
      "local3InputDepth: 640  | local3OutputDepth: 640 (128, 640)\n",
      "Evaluation results:\n",
      "2017-05-06 07:44:16.817533: Total Predictions = 1024\n",
      "2017-05-06 07:44:16.826063: Correct Predictions = 706\n",
      "2017-05-06 07:44:16.834331: Wrong Predictions = 318\n",
      "2017-05-06 07:44:16.842618: precision @ 1 = 0.689\n",
      "2017-05-06 07:44:16.912707: DONE\n",
      "########################################################################################\n",
      "########################################################################################\n",
      "########################################################################################\n"
     ]
    }
   ],
   "source": [
    "from runCIFAR10_Tensorflow import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
