from Sparker.SparkLogProcesser.SparkLogFileHandler import *
from Sparker.Logic.TrainDataHandler import *
from MainSrc.PythonVersionHandler import *
from Sparker.Logic.Trainer import *
from paths import *
import pickle

def loadPickle(fileName):
    file = open(fileName, 'rb')
    return list(pickle.load(file))

def readTrainDataFromPickle(fileName):
    trainData = loadPickle(fileName)
    if isinstance(trainData[0], str):
        trainData = map(lambda x: x.replace('LabeledPoint', ''), trainData)
        trainData = map(eval, trainData)
    trainData = list(map(lambda x: x[1], trainData))
    if isinstance(trainData[0], LabeledPoint):
        trainData = list(map(lambda x: (1.0 if x.label > 0 else 0.0, x.features), trainData))
    else:
        trainData = list(map(lambda x: (1.0 if x[0] > 0 else 0.0, x[1]), trainData))
    print_(fileName, 'has been read successfully by', nowStr()) 
    return trainData

def convertHDFStoPickle(keyword):
    dataTypes = ['_TrainData', '_labeledPairs', '_journey_products']
    for typ in dataTypes[1:-1]:
        fileName = 'all_day_' + keyword + typ
        trainDataFile = joinPath(textTrainDataFolder, fileName + '/part-00000')
        trainData = open(trainDataFile, 'r').readlines()
        #trainData = map(lambda x: x.replace('LabeledPoint', ''), trainData)
        #trainData = map(eval, trainData)
        #trainData = map(lambda x: x[1], trainData)
        #trainData = list(map(lambda x: (1.0 if x[0] > 0 else 0.0, x[1]), trainData))

        #trainData = map(evalProduct, trainData)

        trainData = map(eval, trainData)

        print_(fileName, 'has been read successfully by', nowStr()) 
        
        trainDataFile = trainDataFile.replace('/part-00000', '.txt')
        print_(trainDataFile, 'has been saved successfully by', nowStr())
        fp = open(trainDataFile, 'wb')   #Pickling
        pickle.dump(trainData, fp)

def convertPickleToHDFS(keyword):
    keyword = keyword.replace(' ', '_')
    inputName = 'all_day'
    outputFolder = joinPath(joinPath(textTrainDataFolder, 'HDFS'), 'Day1_' + keyword + '_Data')
    if not os.path.exists(outputFolder):
        os.mkdir(outputFolder)
    dataTypes = ['_TrainData', '_labeledPairs', '_journey_products']
    for typ in dataTypes[1:]:
        fileName = 'all_day_' + keyword + typ
        trainDataFile = joinPath(textTrainDataFolder, fileName + '.txt')
        try:
            trainData = loadPickle(trainDataFile)
        except SyntaxError:
            print_(trainDataFile, 'problem') 
            continue
        #trainData = readTrainDataFromHDFS(trainDataFile)
        print_(trainData[0])
        trainData =  sc_().parallelize(trainData)
        print_(trainDataFile, trainData.count())
        outputFolder = joinPath(outputFolder, fileName)
        if not os.path.exists(outputFolder):
            saveRDDToHDFS(trainData, outputFolder)
        
def oneHot(left, right):
    oneHotVector = [0, 0, 0, 0]
    if left > 0 and right > 0:
        oneHotVector[0] = 1
    elif left > 0 and right < 1:
        oneHotVector[1] = 1
    elif left < 1 and right > 0:
        oneHotVector[2] = 1
    elif left < 1 and right  < 1:
        oneHotVector[3] = 1
    return oneHotVector

featureNames = ['photos', 'soldCount', 'member.feedbackPercentage', 'member.soldCount', 'member.segment', 'subtitleFlag', 'brandNew',
                'cargoInfo.feeType', 'feature.dailyOffer',  'windowOptionFlag',  'buyPrice', 'productCount']
booleanFeatureNames = ['subtitleFlag', 'brandNew', 'cargoInfo.feeType', 'feature.dailyOffer',  'windowOptionFlag']
def oneHotFeatures(leftFeatures, rightFeatures):
    newFeatures = []
    for index in range(len(leftFeatures)):
        fName = featureNames[index]
        if not fName in booleanFeatureNames:
            newFeatures.append(leftFeatures[index] - rightFeatures[index])
        else:
            newFeatures.extend(oneHot(leftFeatures[index], rightFeatures[index]))
    return newFeatures

def generateTrainData(labeledPairs, products):
    print_('train instances is being generated by', nowStr())
    labeledPairs = labeledPairs.map(eval).map(lambda x: [int(id) for id in x[0].split('_')]+[x[1]])
    #products = {id: vector for id, vector in products.collect()}
    #productIds = unique(list(products.keys()))
    #print_(len(productIds))
    #labeledPairs = labeledPairs.filter(lambda x: x[0] in productIds and x[1] in productIds)
    trainData = labeledPairs.map(lambda x: (keyPairIds(x[0], x[1]), LabeledPoint(x[2], oneHotFeatures(products[x[0]], products[x[1]]))))
    print_(trainData.count(), 'train instances have been generated by', nowStr())
    return trainData 

def generateTrainData(labeledPairs, products):
    print_('train instances is being generated by', nowStr())
    labeledPairs = list(map(lambda x: [int(id) for id in x[0].split('_')]+[x[1]], list(map(eval, labeledPairs))))
    products = {id: vector for id, vector in products.collect()}
    productIds = unique(list(products.keys()))
    print_(len(productIds))
    labeledPairs = filter(lambda x: x[0] in productIds and x[1] in productIds, labeledPairs)
    trainData = list(map(lambda x: (keyPairIds(x[0], x[1]), LabeledPoint(x[2], oneHotFeatures(products[x[0]], products[x[1]]))), labeledPairs))
    print_(len(trainData), 'train instances have been generated by', nowStr())
    return trainData 

def generateOneHotTrainData(keyword, inputName, outputFolder):
    keyword = keyword.replace(' ', '_')
    labeledPairsFile = joinPath(outputFolder, inputName + '_' + keyword + '_' + 'labeledPairs.txt')
    labeledPairs = loadPickle(labeledPairsFile)
    #labeledPairs = sc_().parallelize(labeledPairs)
    outputFolder = joinPath(offlineDataHDFSFolder, 'Day1_' + keyword + '_Data')
    journeyProductsFile = joinPath(outputFolder, inputName + '_' + keyword + '_' + 'journey_products')
    products = readProductsFromHDFS(journeyProductsFile)
    trainData = generateTrainData(labeledPairs, products)
    trainDataFile = joinPath(outputFolder, inputName + '_' + keyword + '_' + 'OneHot_TrainData.txt')
    fp = open(trainDataFile, 'wb')   #Pickling
    pickle.dump(trainData, fp)
    print_(trainDataFile, 'has been saved successfully by', nowStr())
    #saveTrainDataToHDFS(trainData, outputFolder, inputName, keyword, '_OneHot')
    return trainData